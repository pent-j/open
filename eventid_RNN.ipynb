{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eventid_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "rdIXMC31b3jU",
        "PSzVu5dkcMLt",
        "MRp1u1IZcTuW",
        "pD0tflqRdIYs",
        "8mWmZsh1d1TD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pent-j/open/blob/master/eventid_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWpPlHl5kyCD",
        "colab_type": "text"
      },
      "source": [
        "# „Ç§„Éô„É≥„Éà„É≠„Ç∞„ÇíRNN„Å´„Åã„Åë„Å¶„ÄÅÁï∞Â∏∏Ê§úÁü•„Åó„Åü„ÅÑ„ÄÇüåÄüçäüåÄüçäüåÄüçäüåÄ\n",
        "\n",
        "## „ÇÑ„Çä„Åü„ÅÑ„Åì„Å®\n",
        "Windows„ÅÆ„Çª„Ç≠„É•„É™„ÉÜ„Ç£„É≠„Ç∞„Çí„ÇÇ„Å®„Å´„ÄÅÁï∞Â∏∏„Å™„É≠„Ç∞„ÅÆÂá∫Êñπ„ÇíÊ§úÁü•„Åó„Åü„ÅÑ„ÄÇ\n",
        "\n",
        "- ÈÄöÂ∏∏„ÅÆ„É≠„Ç∞\n",
        "```\n",
        "(„É≠„Ç∞„Ç§„É≥)(Â§âÊõ¥)(Â§âÊõ¥)(Â§âÊõ¥)(Â§âÊõ¥)(Â§âÊõ¥)(„É≠„Ç∞„Ç¢„Ç¶„Éà)\n",
        "```\n",
        "- Áï∞Â∏∏„Å™„É≠„Ç∞\n",
        "```\n",
        "(„É≠„Ç∞„Ç§„É≥)(„É≠„Ç∞„Ç§„É≥)(„É≠„Ç∞„Ç§„É≥)(„É≠„Ç∞„Ç§„É≥)(„É≠„Ç∞„Ç§„É≥)(„É≠„Ç∞„Ç§„É≥)\n",
        "```\n",
        "\n",
        "## ÂÆüË£Ö„ÅÆÂèÇËÄÉ\n",
        "„Çº„É≠„Åã„Çâ‰Ωú„ÇãDeep Learning ‚ù∑‚Äï‚ÄïËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜÁ∑®\n",
        "https://www.oreilly.co.jp/books/9784873118369/\n",
        "\n",
        "ÊñáÁ´†„Åã„Çâ„ÄÅÊ¨°„Å´Êù•„ÇãÂçòË™û„ÇíÊé®Ê∏¨„Åô„Çã„Ç≥„Éº„Éâ„Å´Â§âÊõ¥„ÇíÂä†„Åà„Åü„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDipYDa5CN3B",
        "colab_type": "text"
      },
      "source": [
        "# ÂÆüË£Ö„Å´„Å§„ÅÑ„Å¶\n",
        "## „Ç§„É≥„Éó„ÉÉ„Éà\n",
        "\n",
        "„Ç§„É≥„Éó„ÉÉ„Éà„Éá„Éº„ÇøÁΩÆ„ÅçÂ†¥Ôºöhttps://raw.githubusercontent.com/pent-j/open/master/\n",
        "- ÂÖ®„É≠„Ç∞Ôºövaio_security.txt\n",
        "- „Éà„É¨„Éº„Éã„É≥„Ç∞„Éá„Éº„ÇøÔºö1_train.txt\n",
        "- „ÉÜ„Çπ„Éà„Éá„Éº„ÇøÔºö2_test.txt\n",
        "- Ê≠£Ëß£„Éá„Éº„ÇøÔºö3_valid.txt\n",
        "\n",
        "## „É¢„Éá„É´\n",
        "„Ç≤„Éº„Éà‰ªò„ÅçRNN\n",
        "\n",
        "![„Åì„Çì„Å™„ÅÆ](https://cdn-ak.f.st-hatena.com/images/fotolife/t/taxa_program/20190108/20190108004224.png)\n",
        "\n",
        "## „É°„É¢\n",
        "ÈÄöÂ∏∏„É≠„Ç∞„ÅåÁï∞Â∏∏„Å´Â§ö„Åô„Åé„Å¶„ÄÅ‰Ωï„Çí„Å®„Å£„Å¶„ÇÇÈÅéÂ≠¶Áøí„Å´„Å™„Å£„Å¶„ÅÑ„ÇãÔºü\n",
        "„Éá„Éº„Çø„Åå„Å†„ÇÅ„Åô„Åé„ÇãÔºüÂÆüË£Ö„ÅåÈñìÈÅï„Å£„Å¶„ÅÑ„ÇãÔºüÂÖ®ÁÑ∂ÂèéÊùü„Åó„Å™„ÅÑ„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP9W9rXrWC2s",
        "colab_type": "text"
      },
      "source": [
        "# „Åì„Çå„Åã„Çâ„ÇÑ„Çã„Åì„Å®\n",
        "\n",
        "- Ê±éÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å∏„ÅÆÂØæÂøúÔºö„Ç´„É≥„ÉûÂå∫Âàá„Çä„ÄÅÊó•Êú¨Ë™ûÊñáÁ´†ÂØæÂøú\n",
        "- „É≠„Ç∞„ÅÆÂ∑Æ„ÅóÊõø„Åà(„Å©„Åì„Åã„ÇâÊåÅ„Å£„Å¶„Åè„Çå„Å∞‚Ä¶SecurityEvent„ÇÑ„ÇÅ„Çã„Åã‚Ä¶Ôºü)\n",
        "- „Ç≥„Éº„Éë„Çπ„Çí„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„Çå„Å∞„ÄÅÁ≤æÂ∫¶„ÇíÊîπÂñÑ„Åß„Åç„ÇãÔºü\n",
        "- Ê±éÁî®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å∏„ÅÆÂØæÂøúÔºö„Ç´„É≥„ÉûÂå∫Âàá„Çä„ÄÅÊó•Êú¨Ë™ûÊñáÁ´†ÂØæÂøú„ÄÅ.evtx„ÇÇ„Åß„Åç„Åü„Çâ„Éª„Éª„Éª\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc_5NWwBvUr9",
        "colab_type": "text"
      },
      "source": [
        "# GPUË®≠ÂÆö/„Ç§„É≥„Éù„Éº„Éà"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZUuXmWPiPUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woLHjxJcAk2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('Use Python3!')\n",
        "import pickle\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ÊúÄÈ†ªÂÄ§Êï∞„Åà‰∏ä„ÅíÁî®\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Slwq8mQXgVR",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uiXYtWbXool",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "class SGD:\n",
        "    '''\n",
        "    Á¢∫ÁéáÁöÑÂãæÈÖçÈôç‰∏ãÊ≥ïÔºàStochastic Gradient DescentÔºâ\n",
        "    '''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for i in range(len(params)):\n",
        "            params[i] -= self.lr * grads[i]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "    '''\n",
        "    Momentum SGD\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = []\n",
        "            for param in params:\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
        "            params[i] += self.v[i]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "    '''\n",
        "    Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = []\n",
        "            for param in params:\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.v[i] *= self.momentum\n",
        "            self.v[i] -= self.lr * grads[i]\n",
        "            params[i] += self.momentum * self.momentum * self.v[i]\n",
        "            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    '''\n",
        "    AdaGrad\n",
        "    '''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = []\n",
        "            for param in params:\n",
        "                self.h.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.h[i] += grads[i] * grads[i]\n",
        "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "    '''\n",
        "    RMSprop\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = []\n",
        "            for param in params:\n",
        "                self.h.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.h[i] *= self.decay_rate\n",
        "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
        "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    '''\n",
        "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
        "    '''\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = [], []\n",
        "            for param in params:\n",
        "                self.m.append(np.zeros_like(param))\n",
        "                self.v.append(np.zeros_like(param))\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "            \n",
        "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQwQDqvYX6cd",
        "colab_type": "text"
      },
      "source": [
        "# np\n",
        "„Éá„Éï„Ç©„É´„Éà„Åß„ÄÅGPU=False„Å´Ë®≠ÂÆö„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh2TX6zVX9H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from config import GPU\n",
        "\n",
        "\n",
        "if GPU:\n",
        "    import cupy as np\n",
        "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
        "    np.add.at = np.scatter_add\n",
        "\n",
        "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
        "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
        "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
        "else:\n",
        "    import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mFgSimYhRw",
        "colab_type": "text"
      },
      "source": [
        "#util\n",
        "\n",
        "„Éá„Éº„ÇøÁ¢∫Ë™çÁî®„Éó„É™„É≥„Éà„ÅÇ„Çä"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJOvhPKNYjvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    '''„Ç≥„Çµ„Ç§„É≥È°û‰ººÂ∫¶„ÅÆÁÆóÂá∫\n",
        "\n",
        "    :param x: „Éô„ÇØ„Éà„É´\n",
        "    :param y: „Éô„ÇØ„Éà„É´\n",
        "    :param eps: ‚Äù0Ââ≤„Çä‚ÄùÈò≤Ê≠¢„ÅÆ„Åü„ÇÅ„ÅÆÂæÆÂ∞èÂÄ§\n",
        "    :return:\n",
        "    '''\n",
        "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    '''È°û‰ººÂçòË™û„ÅÆÊ§úÁ¥¢\n",
        "\n",
        "    :param query: „ÇØ„Ç®„É™Ôºà„ÉÜ„Ç≠„Çπ„ÉàÔºâ\n",
        "    :param word_to_id: ÂçòË™û„Åã„ÇâÂçòË™ûID„Å∏„ÅÆ„Éá„Ç£„ÇØ„Ç∑„Éß„Éä„É™\n",
        "    :param id_to_word: ÂçòË™ûID„Åã„ÇâÂçòË™û„Å∏„ÅÆ„Éá„Ç£„ÇØ„Ç∑„Éß„Éä„É™\n",
        "    :param word_matrix: ÂçòË™û„Éô„ÇØ„Éà„É´„Çí„Åæ„Å®„ÇÅ„ÅüË°åÂàó„ÄÇÂêÑË°å„Å´ÂØæÂøú„Åô„ÇãÂçòË™û„ÅÆ„Éô„ÇØ„Éà„É´„ÅåÊ†ºÁ¥ç„Åï„Çå„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÊÉ≥ÂÆö„Åô„Çã\n",
        "    :param top: ‰∏ä‰Ωç‰Ωï‰Ωç„Åæ„ÅßË°®Á§∫„Åô„Çã„Åã\n",
        "    '''\n",
        "    if query not in word_to_id:\n",
        "        print('%s is not found' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    vocab_size = len(id_to_word)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def convert_one_hot(corpus, vocab_size):\n",
        "    '''one-hotË°®Áèæ„Å∏„ÅÆÂ§âÊèõ\n",
        "\n",
        "    :param corpus: ÂçòË™ûID„ÅÆ„É™„Çπ„ÉàÔºà1Ê¨°ÂÖÉ„ÇÇ„Åó„Åè„ÅØ2Ê¨°ÂÖÉ„ÅÆNumPyÈÖçÂàóÔºâ\n",
        "    :param vocab_size: Ë™ûÂΩôÊï∞\n",
        "    :return: one-hotË°®ÁèæÔºà2Ê¨°ÂÖÉ„ÇÇ„Åó„Åè„ÅØ3Ê¨°ÂÖÉ„ÅÆNumPyÈÖçÂàóÔºâ\n",
        "    '''\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "    '''ÂÖ±Ëµ∑Ë°åÂàó„ÅÆ‰ΩúÊàê\n",
        "\n",
        "    :param corpus: „Ç≥„Éº„Éë„ÇπÔºàÂçòË™ûID„ÅÆ„É™„Çπ„ÉàÔºâ\n",
        "    :param vocab_size:Ë™ûÂΩôÊï∞\n",
        "    :param window_size:„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫Ôºà„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫„Åå1„ÅÆ„Å®„Åç„ÅØ„ÄÅÂçòË™û„ÅÆÂ∑¶Âè≥1ÂçòË™û„Åå„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÔºâ\n",
        "    :return: ÂÖ±Ëµ∑Ë°åÂàó\n",
        "    '''\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_idx = idx - i\n",
        "            right_idx = idx + i\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix\n",
        "\n",
        "\n",
        "def ppmi(C, verbose=False, eps = 1e-8):\n",
        "    '''PPMIÔºàÊ≠£„ÅÆÁõ∏‰∫íÊÉÖÂ†±ÈáèÔºâ„ÅÆ‰ΩúÊàê\n",
        "\n",
        "    :param C: ÂÖ±Ëµ∑Ë°åÂàó\n",
        "    :param verbose: ÈÄ≤Ë°åÁä∂Ê≥Å„ÇíÂá∫Âäõ„Åô„Çã„Åã„Å©„ÅÜ„Åã\n",
        "    :return:\n",
        "    '''\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100) == 0:\n",
        "                    print('%.1f%% done' % (100*cnt/total))\n",
        "    return M\n",
        "\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    '''„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å®„Çø„Éº„Ç≤„ÉÉ„Éà„ÅÆ‰ΩúÊàê\n",
        "\n",
        "    :param corpus: „Ç≥„Éº„Éë„ÇπÔºàÂçòË™ûID„ÅÆ„É™„Çπ„ÉàÔºâ\n",
        "    :param window_size: „Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫Ôºà„Ç¶„Ç£„É≥„Éâ„Ç¶„Çµ„Ç§„Ç∫„Åå1„ÅÆ„Å®„Åç„ÅØ„ÄÅÂçòË™û„ÅÆÂ∑¶Âè≥1ÂçòË™û„Åå„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÔºâ\n",
        "    :return:\n",
        "    '''\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)\n",
        "\n",
        "\n",
        "def to_cpu(x):\n",
        "    import numpy\n",
        "    if type(x) == numpy.ndarray:\n",
        "        return x\n",
        "    return np.asnumpy(x)\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    import cupy\n",
        "    if type(x) == cupy.ndarray:\n",
        "        return x\n",
        "    return cupy.asarray(x)\n",
        "\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(grad ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "    if rate < 1:\n",
        "        for grad in grads:\n",
        "            grad *= rate\n",
        "\n",
        "\n",
        "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
        "    print('evaluating perplexity ...')\n",
        "    corpus_size = len(corpus)\n",
        "    print(\"corpus_size : \",corpus_size)\n",
        "    total_loss, loss_cnt = 0, 0\n",
        "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
        "    jump = (corpus_size - 1) // batch_size\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        time_offset = iters * time_size\n",
        "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
        "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
        "\n",
        "        try:\n",
        "            loss = model.forward(xs, ts, train_flg=False)\n",
        "            #„Éá„Éº„ÇøÁ¢∫Ë™çÁî®\n",
        "            #print(\"xs:\",xs,\",ts:\",ts,\",loss:\",loss)\n",
        "        except TypeError:\n",
        "            loss = model.forward(xs, ts)\n",
        "        total_loss += loss\n",
        "\n",
        "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print('')\n",
        "    print(max_iters)\n",
        "    ppl = np.exp(total_loss / max_iters)\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def eval_seq2seq(model, question, correct, id_to_char,\n",
        "                 verbos=False, is_reverse=False):\n",
        "    correct = correct.flatten()\n",
        "    # È†≠„ÅÆÂå∫Âàá„ÇäÊñáÂ≠ó\n",
        "    start_id = correct[0]\n",
        "    correct = correct[1:]\n",
        "    guess = model.generate(question, start_id, len(correct))\n",
        "\n",
        "    # ÊñáÂ≠óÂàó„Å∏Â§âÊèõ\n",
        "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
        "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
        "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
        "\n",
        "    if verbos:\n",
        "        if is_reverse:\n",
        "            question = question[::-1]\n",
        "\n",
        "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
        "        print('Q', question)\n",
        "        print('T', correct)\n",
        "\n",
        "        is_windows = os.name == 'nt'\n",
        "\n",
        "        if correct == guess:\n",
        "            mark = colors['ok'] + '‚òë' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'O'\n",
        "            print(mark + ' ' + guess)\n",
        "        else:\n",
        "            mark = colors['fail'] + '‚òí' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'X'\n",
        "            print(mark + ' ' + guess)\n",
        "        print('---')\n",
        "\n",
        "    return 1 if guess == correct else 0\n",
        "\n",
        "\n",
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s is not found' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    query_vec = normalize(query_vec)\n",
        "\n",
        "    similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "    if answer is not None:\n",
        "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if np.isnan(similarity[i]):\n",
        "            continue\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    if x.ndim == 2:\n",
        "        s = np.sqrt((x * x).sum(1))\n",
        "        x /= s.reshape((s.shape[0], 1))\n",
        "    elif x.ndim == 1:\n",
        "        s = np.sqrt((x * x).sum())\n",
        "        x /= s\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuMYFkOdZCqn",
        "colab_type": "text"
      },
      "source": [
        "# ‚òÖptb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsC_x7vfZESD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "''''\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train':'ptb.train.txt',\n",
        "    'test':'ptb.test.txt',\n",
        "    'valid':'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'ptb.train.npy',\n",
        "    'test':'ptb.test.npy',\n",
        "    'valid':'ptb.valid.npy'\n",
        "}\n",
        "'''\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/pent-j/open/master/'\n",
        "key_file = {\n",
        "    'train':'1_train.txt',\n",
        "    #'train':'vaio_security.txt',\n",
        "    'test':'2_test.txt',\n",
        "    'valid':'3_valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'1_train.npy',\n",
        "    #'train':'vaio_security.npy',\n",
        "    'test':'2_test.npy',\n",
        "    'valid':'3_valid.npy'\n",
        "}\n",
        "\n",
        "\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "exclude = []#Èô§Â§ñ„Åó„Åü„ÅÑID„ÅÆÈùôÁöÑÊåáÂÆö\n",
        "\n",
        "dataset_dir = \"./\"#os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    except urllib.error.URLError:\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "def load_vocab():\n",
        "    vocab_path = dataset_dir + '/' + vocab_file\n",
        "\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'rb') as f:\n",
        "            word_to_id, id_to_word = pickle.load(f)\n",
        "        \n",
        "        return word_to_id, id_to_word\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    data_type = 'train'\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in word_to_id:\n",
        "            tmp_id = len(word_to_id)\n",
        "            word_to_id[word] = tmp_id\n",
        "            id_to_word[tmp_id] = word\n",
        "\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "\n",
        "    print(word_to_id)\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "def high_f_filter_or(arr,word_to_id,data_type):\n",
        "  \n",
        "    if data_type==\"train\":\n",
        "        print(collections.Counter(arr.tolist()).most_common(3))\n",
        "        exclude.append(collections.Counter(arr.tolist()).most_common(3))\n",
        "        print((exclude))\n",
        "    arr_filtered = arr\n",
        "    for i in exclude[0]:\n",
        "        print(\"i:\",i[0])\n",
        "        print((arr_filtered))\n",
        "        arr_filtered = np.array([int(x) for x in arr_filtered if x != int(i[0])])\n",
        "    return arr_filtered\n",
        "  \n",
        "  \n",
        "  \n",
        "def load_data(data_type='train'):\n",
        "    '''\n",
        "        :param data_type: „Éá„Éº„Çø„ÅÆÁ®ÆÈ°ûÔºö'train' or 'test' or 'valid (val)'\n",
        "        :return:\n",
        "    '''\n",
        "    if data_type == 'val': data_type = 'valid'\n",
        "    save_path = dataset_dir + '/' + save_file[data_type]\n",
        "\n",
        "    word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        corpus = np.load(save_path)\n",
        "        corpus = (high_f_filter_or(corpus,word_to_id,data_type))\n",
        "        \n",
        "        return corpus, word_to_id, id_to_word\n",
        "\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').replace('<eos>','').strip().split()\n",
        "    print(words)\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "    corpus = (high_f_filter_or(corpus,word_to_id,data_type))\n",
        "\n",
        "    np.save(save_path, corpus)\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for data_type in ('train', 'val', 'test'):\n",
        "        load_data(data_type)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1sdZjO-atWq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bCzm48na6q6",
        "colab_type": "text"
      },
      "source": [
        "# basemodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-smH4xTa81v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "class BaseModel:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = None, None\n",
        "\n",
        "    def forward(self, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save_params(self, file_name=None):\n",
        "        if file_name is None:\n",
        "            file_name = self.__class__.__name__ + '.pkl'\n",
        "\n",
        "        params = [p.astype(np.float16) for p in self.params]\n",
        "        if GPU:\n",
        "            params = [to_cpu(p) for p in params]\n",
        "\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=None):\n",
        "        if file_name is None:\n",
        "            file_name = self.__class__.__name__ + '.pkl'\n",
        "\n",
        "        if '/' in file_name:\n",
        "            file_name = file_name.replace('/', os.sep)\n",
        "\n",
        "        if not os.path.exists(file_name):\n",
        "            raise IOError('No file: ' + file_name)\n",
        "\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "\n",
        "        params = [p.astype('f') for p in params]\n",
        "        if GPU:\n",
        "            params = [to_gpu(p) for p in params]\n",
        "\n",
        "        for i, param in enumerate(self.params):\n",
        "            param[...] = params[i]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqBCH-baunw",
        "colab_type": "text"
      },
      "source": [
        "# better rnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhMMXxmCaxsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "class BetterRnnlm(BaseModel):\n",
        "    '''\n",
        "     LSTM„É¨„Ç§„É§„Çí2Â±§Âà©Áî®„Åó„ÄÅÂêÑÂ±§„Å´Dropout„Çí‰Ωø„ÅÜ„É¢„Éá„É´\n",
        "     [1]„ÅßÊèêÊ°à„Åï„Çå„Åü„É¢„Éá„É´„Çí„Éô„Éº„Çπ„Å®„Åó„ÄÅweight tying[2][3]„ÇíÂà©Áî®\n",
        "\n",
        "     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
        "     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
        "     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
        "    '''\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
        "                 hidden_size=650, dropout_ratio=0.5):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
        "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
        "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs, train_flg=False):\n",
        "        for layer in self.drop_layers:\n",
        "            layer.train_flg = train_flg\n",
        "\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts, train_flg=True):\n",
        "        score = self.predict(xs, train_flg)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        for layer in self.lstm_layers:\n",
        "            layer.reset_state()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdIXMC31b3jU",
        "colab_type": "text"
      },
      "source": [
        "# time_layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqGIlsiXb6rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from np import *  # import numpy as np (or import cupy as np)\n",
        "#from layers import *\n",
        "#from functions import sigmoid\n",
        "\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next ** 2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T, dt)\n",
        "        dh_prev = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        '''\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Wx: ÂÖ•Âäõ`x`Áî®„ÅÆÈáç„Åø„Éë„É©„Éº„É°„ÇøÔºà4„Å§ÂàÜ„ÅÆÈáç„Åø„Çí„Åæ„Å®„ÇÅ„ÇãÔºâ\n",
        "        Wh: Èö†„ÇåÁä∂ÊÖã`h`Áî®„ÅÆÈáç„Åø„Éë„É©„É°„Éº„ÇøÔºà4„Å§ÂàÜ„ÅÆÈáç„Åø„Çí„Åæ„Å®„ÇÅ„ÇãÔºâ\n",
        "        b: „Éê„Ç§„Ç¢„ÇπÔºà4„Å§ÂàÜ„ÅÆ„Éê„Ç§„Ç¢„Çπ„Çí„Åæ„Å®„ÇÅ„ÇãÔºâ\n",
        "        '''\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, H = h_prev.shape\n",
        "\n",
        "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
        "\n",
        "        f = A[:, :H]\n",
        "        g = A[:, H:2*H]\n",
        "        i = A[:, 2*H:3*H]\n",
        "        o = A[:, 3*H:]\n",
        "\n",
        "        f = sigmoid(f)\n",
        "        g = np.tanh(g)\n",
        "        i = sigmoid(i)\n",
        "        o = sigmoid(o)\n",
        "\n",
        "        c_next = f * c_prev + g * i\n",
        "        h_next = o * np.tanh(c_next)\n",
        "\n",
        "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def backward(self, dh_next, dc_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
        "\n",
        "        tanh_c_next = np.tanh(c_next)\n",
        "\n",
        "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
        "\n",
        "        dc_prev = ds * f\n",
        "\n",
        "        di = ds * g\n",
        "        df = ds * c_prev\n",
        "        do = dh_next * tanh_c_next\n",
        "        dg = ds * i\n",
        "\n",
        "        di *= i * (1 - i)\n",
        "        df *= f * (1 - f)\n",
        "        do *= o * (1 - o)\n",
        "        dg *= (1 - g ** 2)\n",
        "\n",
        "        dA = np.hstack((df, dg, di, do))\n",
        "\n",
        "        dWh = np.dot(h_prev.T, dA)\n",
        "        dWx = np.dot(x.T, dA)\n",
        "        db = dA.sum(axis=0)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        dx = np.dot(dA, Wx.T)\n",
        "        dh_prev = np.dot(dA, Wh.T)\n",
        "\n",
        "        return dx, dh_prev, dc_prev\n",
        "\n",
        "\n",
        "class TimeLSTM:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.c = None, None\n",
        "        self.dh = None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        H = Wh.shape[0]\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "        if not self.stateful or self.c is None:\n",
        "            self.c = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = LSTM(*self.params)\n",
        "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
        "            hs[:, t, :] = self.h\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D = Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh, dc = 0, 0\n",
        "\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h, c=None):\n",
        "        self.h, self.c = h, c\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h, self.c = None, None\n",
        "\n",
        "\n",
        "class TimeEmbedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.layers = None\n",
        "        self.W = W\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T = xs.shape\n",
        "        V, D = self.W.shape\n",
        "\n",
        "        out = np.empty((N, T, D), dtype='f')\n",
        "        self.layers = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Embedding(self.W)\n",
        "            out[:, t, :] = layer.forward(xs[:, t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, D = dout.shape\n",
        "\n",
        "        grad = 0\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            layer.backward(dout[:, t, :])\n",
        "            grad += layer.grads[0]\n",
        "\n",
        "        self.grads[0][...] = grad\n",
        "        return None\n",
        "\n",
        "\n",
        "class TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        rx = x.reshape(N*T, -1)\n",
        "        out = np.dot(rx, W) + b\n",
        "        self.x = x\n",
        "        return out.reshape(N, T, -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        x = self.x\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        dout = dout.reshape(N*T, -1)\n",
        "        rx = x.reshape(N*T, -1)\n",
        "\n",
        "        db = np.sum(dout, axis=0)\n",
        "        dW = np.dot(rx.T, dout)\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dx = dx.reshape(*x.shape)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "        self.ignore_label = -1\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "\n",
        "        if ts.ndim == 3:  # ÊïôÂ∏´„É©„Éô„É´„Ååone-hot„Éô„ÇØ„Éà„É´„ÅÆÂ†¥Âêà\n",
        "            ts = ts.argmax(axis=2)\n",
        "\n",
        "        mask = (ts != self.ignore_label)\n",
        "\n",
        "        # „Éê„ÉÉ„ÉÅÂàÜ„Å®ÊôÇÁ≥ªÂàóÂàÜ„Çí„Åæ„Å®„ÇÅ„ÇãÔºàreshapeÔºâ\n",
        "        xs = xs.reshape(N * T, V)\n",
        "        ts = ts.reshape(N * T)\n",
        "        mask = mask.reshape(N * T)\n",
        "\n",
        "        ys = softmax(xs)\n",
        "        ls = np.log(ys[np.arange(N * T), ts])\n",
        "        ls *= mask  # ignore_label„Å´Ë©≤ÂΩì„Åô„Çã„Éá„Éº„Çø„ÅØÊêçÂ§±„Çí0„Å´„Åô„Çã\n",
        "        loss = -np.sum(ls)\n",
        "        loss /= mask.sum()\n",
        "\n",
        "        self.cache = (ts, ys, mask, (N, T, V))\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ts, ys, mask, (N, T, V) = self.cache\n",
        "\n",
        "        dx = ys\n",
        "        dx[np.arange(N * T), ts] -= 1\n",
        "        dx *= dout\n",
        "        dx /= mask.sum()\n",
        "        dx *= mask[:, np.newaxis]  # ignore_label„Å´Ë©≤ÂΩì„Åô„Çã„Éá„Éº„Çø„ÅØÂãæÈÖç„Çí0„Å´„Åô„Çã\n",
        "\n",
        "        dx = dx.reshape((N, T, V))\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TimeDropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params, self.grads = [], []\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "        self.train_flg = True\n",
        "\n",
        "    def forward(self, xs):\n",
        "        if self.train_flg:\n",
        "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
        "            scale = 1 / (1.0 - self.dropout_ratio)\n",
        "            self.mask = flg.astype(np.float32) * scale\n",
        "\n",
        "            return xs * self.mask\n",
        "        else:\n",
        "            return xs\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class TimeBiLSTM:\n",
        "    def __init__(self, Wx1, Wh1, b1,\n",
        "                 Wx2, Wh2, b2, stateful=False):\n",
        "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
        "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
        "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
        "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
        "\n",
        "    def forward(self, xs):\n",
        "        o1 = self.forward_lstm.forward(xs)\n",
        "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
        "        o2 = o2[:, ::-1]\n",
        "\n",
        "        out = np.concatenate((o1, o2), axis=2)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        H = dhs.shape[2] // 2\n",
        "        do1 = dhs[:, :, :H]\n",
        "        do2 = dhs[:, :, H:]\n",
        "\n",
        "        dxs1 = self.forward_lstm.backward(do1)\n",
        "        do2 = do2[:, ::-1]\n",
        "        dxs2 = self.backward_lstm.backward(do2)\n",
        "        dxs2 = dxs2[:, ::-1]\n",
        "        dxs = dxs1 + dxs2\n",
        "        return dxs\n",
        "\n",
        "# ====================================================================== #\n",
        "# ‰ª•‰∏ã„Å´Á§∫„Åô„É¨„Ç§„É§„ÅØ„ÄÅÊú¨Êõ∏„ÅßË™¨Êòé„Çí„Åä„Åì„Å™„Å£„Å¶„ÅÑ„Å™„ÅÑ„É¨„Ç§„É§„ÅÆÂÆüË£Ö„ÇÇ„Åó„Åè„ÅØ\n",
        "# Âá¶ÁêÜÈÄüÂ∫¶„Çà„Çä„ÇÇÂàÜ„Åã„Çä„ÇÑ„Åô„Åï„ÇíÂÑ™ÂÖà„Åó„Åü„É¨„Ç§„É§„ÅÆÂÆüË£Ö„Åß„Åô„ÄÇ\n",
        "#\n",
        "# TimeSigmoidWithLoss: ÊôÇÁ≥ªÂàó„Éá„Éº„Çø„ÅÆ„Åü„ÇÅ„ÅÆ„Ç∑„Ç∞„É¢„Ç§„ÉâÊêçÂ§±„É¨„Ç§„É§\n",
        "# GRU: GRU„É¨„Ç§„É§\n",
        "# TimeGRU: ÊôÇÁ≥ªÂàó„Éá„Éº„Çø„ÅÆ„Åü„ÇÅ„ÅÆGRU„É¨„Ç§„É§\n",
        "# BiTimeLSTM: ÂèåÊñπÂêëLSTM„É¨„Ç§„É§\n",
        "# Simple_TimeSoftmaxWithLossÔºöÂçòÁ¥î„Å™TimeSoftmaxWithLoss„É¨„Ç§„É§„ÅÆÂÆüË£Ö\n",
        "# Simple_TimeAffine: ÂçòÁ¥î„Å™TimeAffine„É¨„Ç§„É§„ÅÆÂÆüË£Ö\n",
        "# ====================================================================== #\n",
        "\n",
        "\n",
        "class TimeSigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.xs_shape = None\n",
        "        self.layers = None\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T = xs.shape\n",
        "        self.xs_shape = xs.shape\n",
        "\n",
        "        self.layers = []\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = SigmoidWithLoss()\n",
        "            loss += layer.forward(xs[:, t], ts[:, t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return loss / T\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        N, T = self.xs_shape\n",
        "        dxs = np.empty(self.xs_shape, dtype='f')\n",
        "\n",
        "        dout *= 1/T\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dxs[:, t] = layer.backward(dout)\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "class GRU:\n",
        "    def __init__(self, Wx, Wh):\n",
        "        '''\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Wx: ÂÖ•Âäõ`x`Áî®„ÅÆÈáç„Åø„Éë„É©„Éº„É°„ÇøÔºà3„Å§ÂàÜ„ÅÆÈáç„Åø„Çí„Åæ„Å®„ÇÅ„ÇãÔºâ\n",
        "        Wh: Èö†„ÇåÁä∂ÊÖã`h`Áî®„ÅÆÈáç„Åø„Éë„É©„É°„Éº„ÇøÔºà3„Å§ÂàÜ„ÅÆÈáç„Åø„Çí„Åæ„Å®„ÇÅ„ÇãÔºâ\n",
        "        '''\n",
        "        self.Wx, self.Wh = Wx, Wh\n",
        "        self.dWx, self.dWh = None, None\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        H, H3 = self.Wh.shape\n",
        "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
        "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
        "\n",
        "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
        "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
        "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
        "        h_next = (1-z) * h_prev + z * h_hat\n",
        "\n",
        "        self.cache = (x, h_prev, z, r, h_hat)\n",
        "\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        H, H3 = self.Wh.shape\n",
        "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
        "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
        "        x, h_prev, z, r, h_hat = self.cache\n",
        "\n",
        "        dh_hat =dh_next * z\n",
        "        dh_prev = dh_next * (1-z)\n",
        "\n",
        "        # tanh\n",
        "        dt = dh_hat * (1 - h_hat ** 2)\n",
        "        dWh = np.dot((r * h_prev).T, dt)\n",
        "        dhr = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "        dh_prev += r * dhr\n",
        "\n",
        "        # update gate(z)\n",
        "        dz = dh_next * h_hat - dh_next * h_prev\n",
        "        dt = dz * z * (1-z)\n",
        "        dWhz = np.dot(h_prev.T, dt)\n",
        "        dh_prev += np.dot(dt, Whz.T)\n",
        "        dWxz = np.dot(x.T, dt)\n",
        "        dx += np.dot(dt, Wxz.T)\n",
        "\n",
        "        # rest gate(r)\n",
        "        dr = dhr * h_prev\n",
        "        dt = dr * r * (1-r)\n",
        "        dWhr = np.dot(h_prev.T, dt)\n",
        "        dh_prev += np.dot(dt, Whr.T)\n",
        "        dWxr = np.dot(x.T, dt)\n",
        "        dx += np.dot(dt, Wxr.T)\n",
        "\n",
        "        self.dWx = np.hstack((dWxz, dWxr, dWx))\n",
        "        self.dWh = np.hstack((dWhz, dWhr, dWh))\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeGRU:\n",
        "    def __init__(self, Wx, Wh, stateful=False):\n",
        "        self.Wx, self.Wh = Wx, Wh\n",
        "        selfdWx, self.dWh = None, None\n",
        "        self.layers = None\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T, D = xs.shape\n",
        "        H, H3 = self.Wh.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = GRU(self.Wx, self.Wh)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        N, T, H = dhs.shape\n",
        "        D = self.Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        self.dWx, self.dWh = 0, 0\n",
        "\n",
        "        dh = 0\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "\n",
        "            dxs[:, t, :] = dx\n",
        "            self.dWx += layer.dWx\n",
        "            self.dWh += layer.dWh\n",
        "\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "\n",
        "\n",
        "class Simple_TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "        layers = []\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = SoftmaxWithLoss()\n",
        "            loss += layer.forward(xs[:, t, :], ts[:, t])\n",
        "            layers.append(layer)\n",
        "        loss /= T\n",
        "\n",
        "        self.cache = (layers, xs)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        layers, xs = self.cache\n",
        "        N, T, V = xs.shape\n",
        "        dxs = np.empty(xs.shape, dtype='f')\n",
        "\n",
        "        dout *= 1/T\n",
        "        for t in range(T):\n",
        "            layer = layers[t]\n",
        "            dxs[:, t, :] = layer.backward(dout)\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "class Simple_TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W, self.b = W, b\n",
        "        self.dW, self.db = None, None\n",
        "        self.layers = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T, D = xs.shape\n",
        "        D, M = self.W.shape\n",
        "\n",
        "        self.layers = []\n",
        "        out = np.empty((N, T, M), dtype='f')\n",
        "        for t in range(T):\n",
        "            layer = Affine(self.W, self.b)\n",
        "            out[:, t, :] = layer.forward(xs[:, t, :])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, M = dout.shape\n",
        "        D, M = self.W.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        self.dW, self.db = 0, 0\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dxs[:, t, :] = layer.backward(dout[:, t, :])\n",
        "\n",
        "            self.dW += layer.dW\n",
        "            self.db += layer.db\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSzVu5dkcMLt",
        "colab_type": "text"
      },
      "source": [
        "# functions\n",
        "‰æùÂ≠ò„Å™„Åó"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYFuzfA_cQos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from np import *\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        x /= x.sum(axis=1, keepdims=True)\n",
        "    elif x.ndim == 1:\n",
        "        x = x - np.max(x)\n",
        "        x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # ÊïôÂ∏´„Éá„Éº„Çø„Ååone-hot-vector„ÅÆÂ†¥Âêà„ÄÅÊ≠£Ëß£„É©„Éô„É´„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å´Â§âÊèõ\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRp1u1IZcTuW",
        "colab_type": "text"
      },
      "source": [
        "# layers\n",
        "‰æùÂ≠ò„Å™„Åó"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlnUsupIcVeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from np import *  # import numpy as np\n",
        "#from config import GPU\n",
        "#from functions import softmax, cross_entropy_error\n",
        "\n",
        "\n",
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out = np.dot(x, W) + b\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, b = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.out * dout\n",
        "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "        dx -= self.out * sumdx\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.y = None  # softmax„ÅÆÂá∫Âäõ\n",
        "        self.t = None  # ÊïôÂ∏´„É©„Éô„É´\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "\n",
        "        # ÊïôÂ∏´„É©„Éô„É´„Ååone-hot„Éô„ÇØ„Éà„É´„ÅÆÂ†¥Âêà„ÄÅÊ≠£Ëß£„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å´Â§âÊèõ\n",
        "        if self.t.size == self.y.size:\n",
        "            self.t = self.t.argmax(axis=1)\n",
        "\n",
        "        loss = cross_entropy_error(self.y, self.t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = self.y.copy()\n",
        "        dx[np.arange(batch_size), self.t] -= 1\n",
        "        dx *= dout\n",
        "        dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.loss = None\n",
        "        self.y = None  # sigmoid„ÅÆÂá∫Âäõ\n",
        "        self.t = None  # ÊïôÂ∏´„Éá„Éº„Çø\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = 1 / (1 + np.exp(-x))\n",
        "\n",
        "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = (self.y - self.t) * dout / batch_size\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    '''\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    '''\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params, self.grads = [], []\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        #for i in W:\n",
        "        #    print(i)\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "\n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "        np.add.at(dW, self.idx, dout)\n",
        "        return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD0tflqRdIYs",
        "colab_type": "text"
      },
      "source": [
        "# *rnnlm\n",
        "BaseModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lalmP3BEdL-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#import sys\n",
        "#sys.path.append('..')\n",
        "#from common.time_layers import *\n",
        "#from common.base_model import BaseModel\n",
        "\n",
        "\n",
        "class Rnnlm(BaseModel):\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        print(vocab_size, wordvec_size, hidden_size)\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # Èáç„Åø„ÅÆÂàùÊúüÂåñ\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # „É¨„Ç§„É§„ÅÆÁîüÊàê\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layer = self.layers[1]\n",
        "\n",
        "        # „Åô„Åπ„Å¶„ÅÆÈáç„Åø„Å®ÂãæÈÖç„Çí„É™„Çπ„Éà„Å´„Åæ„Å®„ÇÅ„Çã\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.lstm_layer.reset_state()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mWmZsh1d1TD",
        "colab_type": "text"
      },
      "source": [
        "# trainer\n",
        "‰æùÂ≠ò„Å™„Åó"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lokgl1Uzd6Mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#import sys\n",
        "#sys.path.append('..')\n",
        "#import numpy\n",
        "#import time\n",
        "#import matplotlib.pyplot as plt\n",
        "#from np import *  # import numpy as np\n",
        "#from util import clip_grads\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_list = []\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "        data_size = len(x)\n",
        "        max_iters = data_size // batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            # „Ç∑„É£„ÉÉ„Éï„É´\n",
        "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
        "            x = x[idx]\n",
        "            t = t[idx]\n",
        "\n",
        "            for iters in range(max_iters):\n",
        "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "                # ÂãæÈÖç„ÇíÊ±Ç„ÇÅ„ÄÅ„Éë„É©„É°„Éº„Çø„ÇíÊõ¥Êñ∞\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)  # ÂÖ±Êúâ„Åï„Çå„ÅüÈáç„Åø„Çí1„Å§„Å´ÈõÜÁ¥Ñ\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # Ë©ï‰æ°\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    avg_loss = total_loss / loss_count\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
        "                    self.loss_list.append(float(avg_loss))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = numpy.arange(len(self.loss_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.loss_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class RnnlmTrainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.time_idx = None\n",
        "        self.ppl_list = None\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def get_batch(self, x, t, batch_size, time_size):\n",
        "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
        "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "\n",
        "        data_size = len(x)\n",
        "        jump = data_size // batch_size\n",
        "        offsets = [i * jump for i in range(batch_size)]  # „Éê„ÉÉ„ÉÅ„ÅÆÂêÑ„Çµ„É≥„Éó„É´„ÅÆË™≠„ÅøËæº„ÅøÈñãÂßã‰ΩçÁΩÆ\n",
        "\n",
        "        for time in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
        "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
        "            self.time_idx += 1\n",
        "        return batch_x, batch_t\n",
        "\n",
        "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
        "            max_grad=None, eval_interval=20):\n",
        "        data_size = len(xs)\n",
        "        max_iters = data_size // (batch_size * time_size)\n",
        "        self.time_idx = 0\n",
        "        self.ppl_list = []\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            for iters in range(max_iters):\n",
        "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
        "\n",
        "                # ÂãæÈÖç„ÇíÊ±Ç„ÇÅ„ÄÅ„Éë„É©„É°„Éº„Çø„ÇíÊõ¥Êñ∞\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)  # ÂÖ±Êúâ„Åï„Çå„ÅüÈáç„Åø„Çí1„Å§„Å´ÈõÜÁ¥Ñ\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # „Éë„Éº„Éó„É¨„Ç≠„Ç∑„ÉÜ„Ç£„ÅÆË©ï‰æ°\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    ppl = np.exp(total_loss / loss_count)\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | perplexity %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
        "                    self.ppl_list.append(float(ppl))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = numpy.arange(len(self.ppl_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.ppl_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('perplexity')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def remove_duplicate(params, grads):\n",
        "    '''\n",
        "    „Éë„É©„É°„Éº„ÇøÈÖçÂàó‰∏≠„ÅÆÈáçË§á„Åô„ÇãÈáç„Åø„Çí„Å≤„Å®„Å§„Å´ÈõÜÁ¥Ñ„Åó„ÄÅ\n",
        "    „Åù„ÅÆÈáç„Åø„Å´ÂØæÂøú„Åô„ÇãÂãæÈÖç„ÇíÂä†ÁÆó„Åô„Çã\n",
        "    '''\n",
        "    params, grads = params[:], grads[:]  # copy list\n",
        "\n",
        "    while True:\n",
        "        find_flg = False\n",
        "        L = len(params)\n",
        "\n",
        "        for i in range(0, L - 1):\n",
        "            for j in range(i + 1, L):\n",
        "                # Èáç„Åø„ÇíÂÖ±Êúâ„Åô„ÇãÂ†¥Âêà\n",
        "                if params[i] is params[j]:\n",
        "                    grads[i] += grads[j]  # ÂãæÈÖç„ÅÆÂä†ÁÆó\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "                # Ëª¢ÁΩÆË°åÂàó„Å®„Åó„Å¶Èáç„Åø„ÇíÂÖ±Êúâ„Åô„ÇãÂ†¥ÂêàÔºàweight tyingÔºâ\n",
        "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
        "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
        "                    grads[i] += grads[j].T\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "\n",
        "                if find_flg: break\n",
        "            if find_flg: break\n",
        "\n",
        "        if not find_flg: break\n",
        "\n",
        "    return params, grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzrTEdqUOaxi",
        "colab_type": "text"
      },
      "source": [
        "# ‚òÖmain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQFPxx73Sxx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "#sys.path.append('..')\n",
        "#import config#from common import config\n",
        "# GPU„ÅßÂÆüË°å„Åô„ÇãÂ†¥Âêà„ÅØ‰∏ãË®ò„ÅÆ„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà„ÇíÊ∂àÂéªÔºàË¶ÅcupyÔºâ\n",
        "# ==============================================\n",
        "#GPU = True\n",
        "# ==============================================\n",
        "#from optimizer import SGD\n",
        "#from common.trainer import RnnlmTrainer\n",
        "#from common.util import eval_perplexity, to_gpu\n",
        "#from dataset import ptb\n",
        "#from better_rnnlm import BetterRnnlm\n",
        "\n",
        "\n",
        "# „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅÆË®≠ÂÆö\n",
        "batch_size = 5#20\n",
        "wordvec_size = 300#650\n",
        "hidden_size = 300#650\n",
        "time_size = 10#35\n",
        "lr = 20.0\n",
        "max_epoch = 5#40\n",
        "max_grad = 0.25\n",
        "dropout = 0.5\n",
        "\n",
        "# Â≠¶Áøí„Éá„Éº„Çø„ÅÆË™≠„ÅøËæº„Åø\n",
        "corpus, word_to_id, id_to_word = load_data('train')\n",
        "corpus_val, _, _ = load_data('val')\n",
        "corpus_test, _, _ = load_data('test')\n",
        "#for i in corpus:\n",
        "#  print(i),\n",
        "\n",
        "print(\"gpu setting is \",GPU)\n",
        "if GPU:\n",
        "    corpus = to_gpu(corpus)\n",
        "    corpus_val = to_gpu(corpus_val)\n",
        "    corpus_test = to_gpu(corpus_test)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1:]\n",
        "\n",
        "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
        "optimizer = SGD(lr)\n",
        "trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "best_ppl = float('inf')\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
        "                time_size=time_size, max_grad=max_grad)\n",
        "\n",
        "    model.reset_state()\n",
        "    ppl = eval_perplexity(model, corpus_val)\n",
        "    print('valid perplexity: ', ppl)\n",
        "\n",
        "    if best_ppl > ppl:\n",
        "        best_ppl = ppl\n",
        "        model.save_params()\n",
        "    else:\n",
        "        lr /= 4.0\n",
        "        optimizer.lr = lr\n",
        "\n",
        "    model.reset_state()\n",
        "    print('-' * 50)\n",
        "\n",
        "\n",
        "# „ÉÜ„Çπ„Éà„Éá„Éº„Çø„Åß„ÅÆË©ï‰æ°\n",
        "model.reset_state()\n",
        "ppl_test = eval_perplexity(model, corpus_test)\n",
        "print('test perplexity: ', ppl_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ0xJlqSi19G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((corpus_test))\n",
        "\n",
        "#print(id_to_word[w] for w in corpus_test)\n",
        "\n",
        "for w in corpus_test:\n",
        "  #print(type(str(w)))\n",
        "  #print(id_to_word[int(w)])\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFGOn917ijC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.reset_state()\n",
        "import cupy\n",
        "#ppl_test = eval_perplexity(model, corpus_test)\n",
        "\n",
        "a1 = np.array([4624,1111])\n",
        "tmp_1=np.tile(a1, 200).tolist()\n",
        "\n",
        "#Êñ∞Ë¶èID„ÅÆ„Ç±„Ç¢\n",
        "for i in tmp_1:\n",
        "    try:\n",
        "        word_to_id[i]\n",
        "    except :\n",
        "        tail_id = len(id_to_word)\n",
        "        id_to_word[tail_id] = i\n",
        "        word_to_id[i] = tail_id\n",
        "        print(id_to_word)\n",
        "\n",
        "\n",
        "\n",
        "tmp_1 = np.array([word_to_id[w] for w in tmp_1])\n",
        "print(tmp_1)\n",
        "\n",
        "ppl_test = eval_perplexity(model, tmp_1)\n",
        "print('test perplexity: ', ppl_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}