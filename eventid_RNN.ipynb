{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eventid_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "1Slwq8mQXgVR",
        "yQwQDqvYX6cd",
        "Q9mFgSimYhRw",
        "RuMYFkOdZCqn",
        "6kqBCH-baunw",
        "7bCzm48na6q6",
        "rdIXMC31b3jU",
        "PSzVu5dkcMLt",
        "MRp1u1IZcTuW",
        "pD0tflqRdIYs",
        "7VV4JR4LdU8s",
        "8mWmZsh1d1TD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pent-j/open/blob/master/eventid_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDipYDa5CN3B",
        "colab_type": "text"
      },
      "source": [
        "# イベントログをRNNにかけて、異常検知したい。\n",
        "## インプット\n",
        "- 学習データ：/dataset/train.txt\n",
        "- テストデータ：/dataset/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woLHjxJcAk2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('Use Python3!')\n",
        "import pickle\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgF972u3BUyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import pandas as pd\n",
        " pd.read_csv(\"vaio_security.txt\", encoding=\"SHIFT_JIS\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzrTEdqUOaxi",
        "colab_type": "text"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQFPxx73Sxx5",
        "colab_type": "code",
        "outputId": "74f0cc73-6b88-4e45-ee9b-f93328950e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "#sys.path.append('..')\n",
        "#import config#from common import config\n",
        "# GPUで実行する場合は下記のコメントアウトを消去（要cupy）\n",
        "# ==============================================\n",
        "GPU = True\n",
        "# ==============================================\n",
        "#from optimizer import SGD\n",
        "#from common.trainer import RnnlmTrainer\n",
        "#from common.util import eval_perplexity, to_gpu\n",
        "#from dataset import ptb\n",
        "#from better_rnnlm import BetterRnnlm\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "batch_size = 100#20\n",
        "wordvec_size = 50#650\n",
        "hidden_size = 50#650\n",
        "time_size = 5#35\n",
        "lr = 20.0\n",
        "max_epoch = 5#40\n",
        "max_grad = 0.25\n",
        "dropout = 0.5\n",
        "\n",
        "# 学習データの読み込み\n",
        "corpus, word_to_id, id_to_word = load_data('train')\n",
        "corpus_val, _, _ = load_data('val')\n",
        "corpus_test, _, _ = load_data('test')\n",
        "print(\"corpus:\",corpus)\n",
        "\n",
        "print(\"gpu setting is \",GPU)\n",
        "if GPU:\n",
        "    corpus = to_gpu(corpus)\n",
        "    corpus_val = to_gpu(corpus_val)\n",
        "    corpus_test = to_gpu(corpus_test)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1:]\n",
        "print(xs)\n",
        "\n",
        "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
        "optimizer = SGD(lr)\n",
        "trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "best_ppl = float('inf')\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
        "                time_size=time_size, max_grad=max_grad)\n",
        "\n",
        "    model.reset_state()\n",
        "    ppl = eval_perplexity(model, corpus_val)\n",
        "    print('valid perplexity: ', ppl)\n",
        "\n",
        "    if best_ppl > ppl:\n",
        "        best_ppl = ppl\n",
        "        model.save_params()\n",
        "    else:\n",
        "        lr /= 4.0\n",
        "        optimizer.lr = lr\n",
        "\n",
        "    model.reset_state()\n",
        "    print('-' * 50)\n",
        "\n",
        "\n",
        "# テストデータでの評価\n",
        "model.reset_state()\n",
        "ppl_test = eval_perplexity(model, corpus_test)\n",
        "print('test perplexity: ', ppl_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus: [ 0  1  2 ... 39 26 24]\n",
            "gpu setting is  True\n",
            "[ 0  1  2 ... 64 39 26]\n",
            "| epoch 1 |  iter 1 / 1859 | time 0[s] | perplexity 9999.94\n",
            "| epoch 1 |  iter 21 / 1859 | time 3[s] | perplexity 2338.77\n",
            "| epoch 1 |  iter 41 / 1859 | time 6[s] | perplexity 1257.34\n",
            "| epoch 1 |  iter 61 / 1859 | time 10[s] | perplexity 1091.33\n",
            "| epoch 1 |  iter 81 / 1859 | time 14[s] | perplexity 993.32\n",
            "| epoch 1 |  iter 101 / 1859 | time 18[s] | perplexity 846.74\n",
            "| epoch 1 |  iter 121 / 1859 | time 21[s] | perplexity 799.40\n",
            "| epoch 1 |  iter 141 / 1859 | time 25[s] | perplexity 773.25\n",
            "| epoch 1 |  iter 161 / 1859 | time 28[s] | perplexity 742.12\n",
            "| epoch 1 |  iter 181 / 1859 | time 32[s] | perplexity 688.82\n",
            "| epoch 1 |  iter 201 / 1859 | time 35[s] | perplexity 649.80\n",
            "| epoch 1 |  iter 221 / 1859 | time 38[s] | perplexity 644.29\n",
            "| epoch 1 |  iter 241 / 1859 | time 42[s] | perplexity 568.13\n",
            "| epoch 1 |  iter 261 / 1859 | time 45[s] | perplexity 578.66\n",
            "| epoch 1 |  iter 281 / 1859 | time 48[s] | perplexity 599.31\n",
            "| epoch 1 |  iter 301 / 1859 | time 52[s] | perplexity 562.22\n",
            "| epoch 1 |  iter 321 / 1859 | time 56[s] | perplexity 545.48\n",
            "| epoch 1 |  iter 341 / 1859 | time 60[s] | perplexity 537.92\n",
            "| epoch 1 |  iter 361 / 1859 | time 63[s] | perplexity 544.95\n",
            "| epoch 1 |  iter 381 / 1859 | time 67[s] | perplexity 536.73\n",
            "| epoch 1 |  iter 401 / 1859 | time 70[s] | perplexity 525.15\n",
            "| epoch 1 |  iter 421 / 1859 | time 74[s] | perplexity 522.18\n",
            "| epoch 1 |  iter 441 / 1859 | time 77[s] | perplexity 542.86\n",
            "| epoch 1 |  iter 461 / 1859 | time 80[s] | perplexity 524.25\n",
            "| epoch 1 |  iter 481 / 1859 | time 84[s] | perplexity 483.77\n",
            "| epoch 1 |  iter 501 / 1859 | time 87[s] | perplexity 493.77\n",
            "| epoch 1 |  iter 521 / 1859 | time 90[s] | perplexity 507.68\n",
            "| epoch 1 |  iter 541 / 1859 | time 94[s] | perplexity 504.68\n",
            "| epoch 1 |  iter 561 / 1859 | time 97[s] | perplexity 497.74\n",
            "| epoch 1 |  iter 581 / 1859 | time 101[s] | perplexity 478.36\n",
            "| epoch 1 |  iter 601 / 1859 | time 104[s] | perplexity 459.89\n",
            "| epoch 1 |  iter 621 / 1859 | time 107[s] | perplexity 474.82\n",
            "| epoch 1 |  iter 641 / 1859 | time 111[s] | perplexity 443.46\n",
            "| epoch 1 |  iter 661 / 1859 | time 114[s] | perplexity 419.45\n",
            "| epoch 1 |  iter 681 / 1859 | time 118[s] | perplexity 445.32\n",
            "| epoch 1 |  iter 701 / 1859 | time 121[s] | perplexity 445.49\n",
            "| epoch 1 |  iter 721 / 1859 | time 124[s] | perplexity 424.24\n",
            "| epoch 1 |  iter 741 / 1859 | time 128[s] | perplexity 423.86\n",
            "| epoch 1 |  iter 761 / 1859 | time 131[s] | perplexity 434.07\n",
            "| epoch 1 |  iter 781 / 1859 | time 135[s] | perplexity 422.98\n",
            "| epoch 1 |  iter 801 / 1859 | time 139[s] | perplexity 422.94\n",
            "| epoch 1 |  iter 821 / 1859 | time 143[s] | perplexity 427.01\n",
            "| epoch 1 |  iter 841 / 1859 | time 146[s] | perplexity 442.33\n",
            "| epoch 1 |  iter 861 / 1859 | time 150[s] | perplexity 395.70\n",
            "| epoch 1 |  iter 881 / 1859 | time 153[s] | perplexity 436.06\n",
            "| epoch 1 |  iter 901 / 1859 | time 157[s] | perplexity 377.47\n",
            "| epoch 1 |  iter 921 / 1859 | time 160[s] | perplexity 403.77\n",
            "| epoch 1 |  iter 941 / 1859 | time 163[s] | perplexity 415.69\n",
            "| epoch 1 |  iter 961 / 1859 | time 167[s] | perplexity 408.94\n",
            "| epoch 1 |  iter 981 / 1859 | time 170[s] | perplexity 383.14\n",
            "| epoch 1 |  iter 1001 / 1859 | time 173[s] | perplexity 379.93\n",
            "| epoch 1 |  iter 1021 / 1859 | time 177[s] | perplexity 390.17\n",
            "| epoch 1 |  iter 1041 / 1859 | time 180[s] | perplexity 411.02\n",
            "| epoch 1 |  iter 1061 / 1859 | time 184[s] | perplexity 404.60\n",
            "| epoch 1 |  iter 1081 / 1859 | time 187[s] | perplexity 400.97\n",
            "| epoch 1 |  iter 1101 / 1859 | time 190[s] | perplexity 414.62\n",
            "| epoch 1 |  iter 1121 / 1859 | time 194[s] | perplexity 398.16\n",
            "| epoch 1 |  iter 1141 / 1859 | time 197[s] | perplexity 412.37\n",
            "| epoch 1 |  iter 1161 / 1859 | time 200[s] | perplexity 375.64\n",
            "| epoch 1 |  iter 1181 / 1859 | time 204[s] | perplexity 385.41\n",
            "| epoch 1 |  iter 1201 / 1859 | time 207[s] | perplexity 411.56\n",
            "| epoch 1 |  iter 1221 / 1859 | time 210[s] | perplexity 392.91\n",
            "| epoch 1 |  iter 1241 / 1859 | time 214[s] | perplexity 425.62\n",
            "| epoch 1 |  iter 1261 / 1859 | time 218[s] | perplexity 413.90\n",
            "| epoch 1 |  iter 1281 / 1859 | time 222[s] | perplexity 378.98\n",
            "| epoch 1 |  iter 1301 / 1859 | time 225[s] | perplexity 391.98\n",
            "| epoch 1 |  iter 1321 / 1859 | time 229[s] | perplexity 357.60\n",
            "| epoch 1 |  iter 1341 / 1859 | time 232[s] | perplexity 356.45\n",
            "| epoch 1 |  iter 1361 / 1859 | time 236[s] | perplexity 358.74\n",
            "| epoch 1 |  iter 1381 / 1859 | time 239[s] | perplexity 373.29\n",
            "| epoch 1 |  iter 1401 / 1859 | time 242[s] | perplexity 368.93\n",
            "| epoch 1 |  iter 1421 / 1859 | time 246[s] | perplexity 374.20\n",
            "| epoch 1 |  iter 1441 / 1859 | time 249[s] | perplexity 381.03\n",
            "| epoch 1 |  iter 1461 / 1859 | time 252[s] | perplexity 376.42\n",
            "| epoch 1 |  iter 1481 / 1859 | time 256[s] | perplexity 371.19\n",
            "| epoch 1 |  iter 1501 / 1859 | time 259[s] | perplexity 378.75\n",
            "| epoch 1 |  iter 1521 / 1859 | time 263[s] | perplexity 355.79\n",
            "| epoch 1 |  iter 1541 / 1859 | time 267[s] | perplexity 364.83\n",
            "| epoch 1 |  iter 1561 / 1859 | time 270[s] | perplexity 382.16\n",
            "| epoch 1 |  iter 1581 / 1859 | time 274[s] | perplexity 388.37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Slwq8mQXgVR",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uiXYtWbXool",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "class SGD:\n",
        "    '''\n",
        "    確率的勾配降下法（Stochastic Gradient Descent）\n",
        "    '''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for i in range(len(params)):\n",
        "            params[i] -= self.lr * grads[i]\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "    '''\n",
        "    Momentum SGD\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = []\n",
        "            for param in params:\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
        "            params[i] += self.v[i]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "    '''\n",
        "    Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = []\n",
        "            for param in params:\n",
        "                self.v.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.v[i] *= self.momentum\n",
        "            self.v[i] -= self.lr * grads[i]\n",
        "            params[i] += self.momentum * self.momentum * self.v[i]\n",
        "            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "    '''\n",
        "    AdaGrad\n",
        "    '''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = []\n",
        "            for param in params:\n",
        "                self.h.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.h[i] += grads[i] * grads[i]\n",
        "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "    '''\n",
        "    RMSprop\n",
        "    '''\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = []\n",
        "            for param in params:\n",
        "                self.h.append(np.zeros_like(param))\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.h[i] *= self.decay_rate\n",
        "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
        "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "    '''\n",
        "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
        "    '''\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = [], []\n",
        "            for param in params:\n",
        "                self.m.append(np.zeros_like(param))\n",
        "                self.v.append(np.zeros_like(param))\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for i in range(len(params)):\n",
        "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "            \n",
        "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQwQDqvYX6cd",
        "colab_type": "text"
      },
      "source": [
        "# np/GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh2TX6zVX9H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from config import GPU\n",
        "\n",
        "\n",
        "if GPU:\n",
        "    import cupy as np\n",
        "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
        "    np.add.at = np.scatter_add\n",
        "\n",
        "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
        "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
        "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
        "else:\n",
        "    import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdUDCGzXYIxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "GPU = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mFgSimYhRw",
        "colab_type": "text"
      },
      "source": [
        "#util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJOvhPKNYjvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    '''コサイン類似度の算出\n",
        "\n",
        "    :param x: ベクトル\n",
        "    :param y: ベクトル\n",
        "    :param eps: ”0割り”防止のための微小値\n",
        "    :return:\n",
        "    '''\n",
        "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    '''類似単語の検索\n",
        "\n",
        "    :param query: クエリ（テキスト）\n",
        "    :param word_to_id: 単語から単語IDへのディクショナリ\n",
        "    :param id_to_word: 単語IDから単語へのディクショナリ\n",
        "    :param word_matrix: 単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されていることを想定する\n",
        "    :param top: 上位何位まで表示するか\n",
        "    '''\n",
        "    if query not in word_to_id:\n",
        "        print('%s is not found' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    vocab_size = len(id_to_word)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def convert_one_hot(corpus, vocab_size):\n",
        "    '''one-hot表現への変換\n",
        "\n",
        "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
        "    :param vocab_size: 語彙数\n",
        "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
        "    '''\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "    '''共起行列の作成\n",
        "\n",
        "    :param corpus: コーパス（単語IDのリスト）\n",
        "    :param vocab_size:語彙数\n",
        "    :param window_size:ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
        "    :return: 共起行列\n",
        "    '''\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_idx = idx - i\n",
        "            right_idx = idx + i\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix\n",
        "\n",
        "\n",
        "def ppmi(C, verbose=False, eps = 1e-8):\n",
        "    '''PPMI（正の相互情報量）の作成\n",
        "\n",
        "    :param C: 共起行列\n",
        "    :param verbose: 進行状況を出力するかどうか\n",
        "    :return:\n",
        "    '''\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100) == 0:\n",
        "                    print('%.1f%% done' % (100*cnt/total))\n",
        "    return M\n",
        "\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    '''コンテキストとターゲットの作成\n",
        "\n",
        "    :param corpus: コーパス（単語IDのリスト）\n",
        "    :param window_size: ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
        "    :return:\n",
        "    '''\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)\n",
        "\n",
        "\n",
        "def to_cpu(x):\n",
        "    import numpy\n",
        "    if type(x) == numpy.ndarray:\n",
        "        return x\n",
        "    return np.asnumpy(x)\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    import cupy\n",
        "    if type(x) == cupy.ndarray:\n",
        "        return x\n",
        "    return cupy.asarray(x)\n",
        "\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(grad ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "    if rate < 1:\n",
        "        for grad in grads:\n",
        "            grad *= rate\n",
        "\n",
        "\n",
        "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
        "    print('evaluating perplexity ...')\n",
        "    corpus_size = len(corpus)\n",
        "    total_loss, loss_cnt = 0, 0\n",
        "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
        "    jump = (corpus_size - 1) // batch_size\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        time_offset = iters * time_size\n",
        "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
        "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
        "\n",
        "        try:\n",
        "            loss = model.forward(xs, ts, train_flg=False)\n",
        "            print(\"xs:\",xs,\",ts:\",ts,\",loss:\",loss)\n",
        "        except TypeError:\n",
        "            loss = model.forward(xs, ts)\n",
        "        total_loss += loss\n",
        "\n",
        "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print('')\n",
        "    ppl = np.exp(total_loss / max_iters)\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def eval_seq2seq(model, question, correct, id_to_char,\n",
        "                 verbos=False, is_reverse=False):\n",
        "    correct = correct.flatten()\n",
        "    # 頭の区切り文字\n",
        "    start_id = correct[0]\n",
        "    correct = correct[1:]\n",
        "    guess = model.generate(question, start_id, len(correct))\n",
        "\n",
        "    # 文字列へ変換\n",
        "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
        "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
        "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
        "\n",
        "    if verbos:\n",
        "        if is_reverse:\n",
        "            question = question[::-1]\n",
        "\n",
        "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
        "        print('Q', question)\n",
        "        print('T', correct)\n",
        "\n",
        "        is_windows = os.name == 'nt'\n",
        "\n",
        "        if correct == guess:\n",
        "            mark = colors['ok'] + '☑' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'O'\n",
        "            print(mark + ' ' + guess)\n",
        "        else:\n",
        "            mark = colors['fail'] + '☒' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'X'\n",
        "            print(mark + ' ' + guess)\n",
        "        print('---')\n",
        "\n",
        "    return 1 if guess == correct else 0\n",
        "\n",
        "\n",
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s is not found' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    query_vec = normalize(query_vec)\n",
        "\n",
        "    similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "    if answer is not None:\n",
        "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if np.isnan(similarity[i]):\n",
        "            continue\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    if x.ndim == 2:\n",
        "        s = np.sqrt((x * x).sum(1))\n",
        "        x /= s.reshape((s.shape[0], 1))\n",
        "    elif x.ndim == 1:\n",
        "        s = np.sqrt((x * x).sum())\n",
        "        x /= s\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuMYFkOdZCqn",
        "colab_type": "text"
      },
      "source": [
        "# ptb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsC_x7vfZESD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "50579c8e-02f0-4683-c437-b868893a8221"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train':'ptb.train.txt',\n",
        "    'test':'ptb.test.txt',\n",
        "    'valid':'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'ptb.train.npy',\n",
        "    'test':'ptb.test.npy',\n",
        "    'valid':'ptb.valid.npy'\n",
        "}\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "\n",
        "dataset_dir = \"./\"#os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    except urllib.error.URLError:\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "def load_vocab():\n",
        "    vocab_path = dataset_dir + '/' + vocab_file\n",
        "\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'rb') as f:\n",
        "            word_to_id, id_to_word = pickle.load(f)\n",
        "        return word_to_id, id_to_word\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    data_type = 'train'\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in word_to_id:\n",
        "            tmp_id = len(word_to_id)\n",
        "            word_to_id[word] = tmp_id\n",
        "            id_to_word[tmp_id] = word\n",
        "\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def load_data(data_type='train'):\n",
        "    '''\n",
        "        :param data_type: データの種類：'train' or 'test' or 'valid (val)'\n",
        "        :return:\n",
        "    '''\n",
        "    if data_type == 'val': data_type = 'valid'\n",
        "    save_path = dataset_dir + '/' + save_file[data_type]\n",
        "\n",
        "    word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        corpus = np.load(save_path)\n",
        "        return corpus, word_to_id, id_to_word\n",
        "\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    np.save(save_path, corpus)\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for data_type in ('train', 'val', 'test'):\n",
        "        load_data(data_type)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ptb.train.txt ... \n",
            "Done\n",
            "Downloading ptb.valid.txt ... \n",
            "Done\n",
            "Downloading ptb.test.txt ... \n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1sdZjO-atWq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqBCH-baunw",
        "colab_type": "text"
      },
      "source": [
        "# better rnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhMMXxmCaxsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "class BetterRnnlm(BaseModel):\n",
        "    '''\n",
        "     LSTMレイヤを2層利用し、各層にDropoutを使うモデル\n",
        "     [1]で提案されたモデルをベースとし、weight tying[2][3]を利用\n",
        "\n",
        "     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
        "     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
        "     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
        "    '''\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
        "                 hidden_size=650, dropout_ratio=0.5):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
        "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
        "            TimeDropout(dropout_ratio),\n",
        "            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
        "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs, train_flg=False):\n",
        "        for layer in self.drop_layers:\n",
        "            layer.train_flg = train_flg\n",
        "\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts, train_flg=True):\n",
        "        score = self.predict(xs, train_flg)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        for layer in self.lstm_layers:\n",
        "            layer.reset_state()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bCzm48na6q6",
        "colab_type": "text"
      },
      "source": [
        "# basemodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-smH4xTa81v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "class BaseModel:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = None, None\n",
        "\n",
        "    def forward(self, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save_params(self, file_name=None):\n",
        "        if file_name is None:\n",
        "            file_name = self.__class__.__name__ + '.pkl'\n",
        "\n",
        "        params = [p.astype(np.float16) for p in self.params]\n",
        "        if GPU:\n",
        "            params = [to_cpu(p) for p in params]\n",
        "\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=None):\n",
        "        if file_name is None:\n",
        "            file_name = self.__class__.__name__ + '.pkl'\n",
        "\n",
        "        if '/' in file_name:\n",
        "            file_name = file_name.replace('/', os.sep)\n",
        "\n",
        "        if not os.path.exists(file_name):\n",
        "            raise IOError('No file: ' + file_name)\n",
        "\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "\n",
        "        params = [p.astype('f') for p in params]\n",
        "        if GPU:\n",
        "            params = [to_gpu(p) for p in params]\n",
        "\n",
        "        for i, param in enumerate(self.params):\n",
        "            param[...] = params[i]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdIXMC31b3jU",
        "colab_type": "text"
      },
      "source": [
        "# time_layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqGIlsiXb6rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from np import *  # import numpy as np (or import cupy as np)\n",
        "#from layers import *\n",
        "#from functions import sigmoid\n",
        "\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next ** 2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T, dt)\n",
        "        dh_prev = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "\n",
        "\n",
        "class LSTM:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        '''\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Wx: 入力`x`用の重みパラーメタ（4つ分の重みをまとめる）\n",
        "        Wh: 隠れ状態`h`用の重みパラメータ（4つ分の重みをまとめる）\n",
        "        b: バイアス（4つ分のバイアスをまとめる）\n",
        "        '''\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, H = h_prev.shape\n",
        "\n",
        "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
        "\n",
        "        f = A[:, :H]\n",
        "        g = A[:, H:2*H]\n",
        "        i = A[:, 2*H:3*H]\n",
        "        o = A[:, 3*H:]\n",
        "\n",
        "        f = sigmoid(f)\n",
        "        g = np.tanh(g)\n",
        "        i = sigmoid(i)\n",
        "        o = sigmoid(o)\n",
        "\n",
        "        c_next = f * c_prev + g * i\n",
        "        h_next = o * np.tanh(c_next)\n",
        "\n",
        "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def backward(self, dh_next, dc_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
        "\n",
        "        tanh_c_next = np.tanh(c_next)\n",
        "\n",
        "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
        "\n",
        "        dc_prev = ds * f\n",
        "\n",
        "        di = ds * g\n",
        "        df = ds * c_prev\n",
        "        do = dh_next * tanh_c_next\n",
        "        dg = ds * i\n",
        "\n",
        "        di *= i * (1 - i)\n",
        "        df *= f * (1 - f)\n",
        "        do *= o * (1 - o)\n",
        "        dg *= (1 - g ** 2)\n",
        "\n",
        "        dA = np.hstack((df, dg, di, do))\n",
        "\n",
        "        dWh = np.dot(h_prev.T, dA)\n",
        "        dWx = np.dot(x.T, dA)\n",
        "        db = dA.sum(axis=0)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        dx = np.dot(dA, Wx.T)\n",
        "        dh_prev = np.dot(dA, Wh.T)\n",
        "\n",
        "        return dx, dh_prev, dc_prev\n",
        "\n",
        "\n",
        "class TimeLSTM:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.c = None, None\n",
        "        self.dh = None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        H = Wh.shape[0]\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "        if not self.stateful or self.c is None:\n",
        "            self.c = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = LSTM(*self.params)\n",
        "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
        "            hs[:, t, :] = self.h\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D = Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh, dc = 0, 0\n",
        "\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h, c=None):\n",
        "        self.h, self.c = h, c\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h, self.c = None, None\n",
        "\n",
        "\n",
        "class TimeEmbedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.layers = None\n",
        "        self.W = W\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T = xs.shape\n",
        "        V, D = self.W.shape\n",
        "\n",
        "        out = np.empty((N, T, D), dtype='f')\n",
        "        self.layers = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Embedding(self.W)\n",
        "            out[:, t, :] = layer.forward(xs[:, t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, D = dout.shape\n",
        "\n",
        "        grad = 0\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            layer.backward(dout[:, t, :])\n",
        "            grad += layer.grads[0]\n",
        "\n",
        "        self.grads[0][...] = grad\n",
        "        return None\n",
        "\n",
        "\n",
        "class TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        rx = x.reshape(N*T, -1)\n",
        "        out = np.dot(rx, W) + b\n",
        "        self.x = x\n",
        "        return out.reshape(N, T, -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        x = self.x\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        dout = dout.reshape(N*T, -1)\n",
        "        rx = x.reshape(N*T, -1)\n",
        "\n",
        "        db = np.sum(dout, axis=0)\n",
        "        dW = np.dot(rx.T, dout)\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dx = dx.reshape(*x.shape)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "        self.ignore_label = -1\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "\n",
        "        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n",
        "            ts = ts.argmax(axis=2)\n",
        "\n",
        "        mask = (ts != self.ignore_label)\n",
        "\n",
        "        # バッチ分と時系列分をまとめる（reshape）\n",
        "        xs = xs.reshape(N * T, V)\n",
        "        ts = ts.reshape(N * T)\n",
        "        mask = mask.reshape(N * T)\n",
        "\n",
        "        ys = softmax(xs)\n",
        "        ls = np.log(ys[np.arange(N * T), ts])\n",
        "        ls *= mask  # ignore_labelに該当するデータは損失を0にする\n",
        "        loss = -np.sum(ls)\n",
        "        loss /= mask.sum()\n",
        "\n",
        "        self.cache = (ts, ys, mask, (N, T, V))\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ts, ys, mask, (N, T, V) = self.cache\n",
        "\n",
        "        dx = ys\n",
        "        dx[np.arange(N * T), ts] -= 1\n",
        "        dx *= dout\n",
        "        dx /= mask.sum()\n",
        "        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n",
        "\n",
        "        dx = dx.reshape((N, T, V))\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class TimeDropout:\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params, self.grads = [], []\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "        self.train_flg = True\n",
        "\n",
        "    def forward(self, xs):\n",
        "        if self.train_flg:\n",
        "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
        "            scale = 1 / (1.0 - self.dropout_ratio)\n",
        "            self.mask = flg.astype(np.float32) * scale\n",
        "\n",
        "            return xs * self.mask\n",
        "        else:\n",
        "            return xs\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class TimeBiLSTM:\n",
        "    def __init__(self, Wx1, Wh1, b1,\n",
        "                 Wx2, Wh2, b2, stateful=False):\n",
        "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
        "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
        "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
        "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
        "\n",
        "    def forward(self, xs):\n",
        "        o1 = self.forward_lstm.forward(xs)\n",
        "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
        "        o2 = o2[:, ::-1]\n",
        "\n",
        "        out = np.concatenate((o1, o2), axis=2)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        H = dhs.shape[2] // 2\n",
        "        do1 = dhs[:, :, :H]\n",
        "        do2 = dhs[:, :, H:]\n",
        "\n",
        "        dxs1 = self.forward_lstm.backward(do1)\n",
        "        do2 = do2[:, ::-1]\n",
        "        dxs2 = self.backward_lstm.backward(do2)\n",
        "        dxs2 = dxs2[:, ::-1]\n",
        "        dxs = dxs1 + dxs2\n",
        "        return dxs\n",
        "\n",
        "# ====================================================================== #\n",
        "# 以下に示すレイヤは、本書で説明をおこなっていないレイヤの実装もしくは\n",
        "# 処理速度よりも分かりやすさを優先したレイヤの実装です。\n",
        "#\n",
        "# TimeSigmoidWithLoss: 時系列データのためのシグモイド損失レイヤ\n",
        "# GRU: GRUレイヤ\n",
        "# TimeGRU: 時系列データのためのGRUレイヤ\n",
        "# BiTimeLSTM: 双方向LSTMレイヤ\n",
        "# Simple_TimeSoftmaxWithLoss：単純なTimeSoftmaxWithLossレイヤの実装\n",
        "# Simple_TimeAffine: 単純なTimeAffineレイヤの実装\n",
        "# ====================================================================== #\n",
        "\n",
        "\n",
        "class TimeSigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.xs_shape = None\n",
        "        self.layers = None\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T = xs.shape\n",
        "        self.xs_shape = xs.shape\n",
        "\n",
        "        self.layers = []\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = SigmoidWithLoss()\n",
        "            loss += layer.forward(xs[:, t], ts[:, t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return loss / T\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        N, T = self.xs_shape\n",
        "        dxs = np.empty(self.xs_shape, dtype='f')\n",
        "\n",
        "        dout *= 1/T\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dxs[:, t] = layer.backward(dout)\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "class GRU:\n",
        "    def __init__(self, Wx, Wh):\n",
        "        '''\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Wx: 入力`x`用の重みパラーメタ（3つ分の重みをまとめる）\n",
        "        Wh: 隠れ状態`h`用の重みパラメータ（3つ分の重みをまとめる）\n",
        "        '''\n",
        "        self.Wx, self.Wh = Wx, Wh\n",
        "        self.dWx, self.dWh = None, None\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        H, H3 = self.Wh.shape\n",
        "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
        "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
        "\n",
        "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
        "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
        "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
        "        h_next = (1-z) * h_prev + z * h_hat\n",
        "\n",
        "        self.cache = (x, h_prev, z, r, h_hat)\n",
        "\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        H, H3 = self.Wh.shape\n",
        "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
        "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
        "        x, h_prev, z, r, h_hat = self.cache\n",
        "\n",
        "        dh_hat =dh_next * z\n",
        "        dh_prev = dh_next * (1-z)\n",
        "\n",
        "        # tanh\n",
        "        dt = dh_hat * (1 - h_hat ** 2)\n",
        "        dWh = np.dot((r * h_prev).T, dt)\n",
        "        dhr = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "        dh_prev += r * dhr\n",
        "\n",
        "        # update gate(z)\n",
        "        dz = dh_next * h_hat - dh_next * h_prev\n",
        "        dt = dz * z * (1-z)\n",
        "        dWhz = np.dot(h_prev.T, dt)\n",
        "        dh_prev += np.dot(dt, Whz.T)\n",
        "        dWxz = np.dot(x.T, dt)\n",
        "        dx += np.dot(dt, Wxz.T)\n",
        "\n",
        "        # rest gate(r)\n",
        "        dr = dhr * h_prev\n",
        "        dt = dr * r * (1-r)\n",
        "        dWhr = np.dot(h_prev.T, dt)\n",
        "        dh_prev += np.dot(dt, Whr.T)\n",
        "        dWxr = np.dot(x.T, dt)\n",
        "        dx += np.dot(dt, Wxr.T)\n",
        "\n",
        "        self.dWx = np.hstack((dWxz, dWxr, dWx))\n",
        "        self.dWh = np.hstack((dWhz, dWhr, dWh))\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeGRU:\n",
        "    def __init__(self, Wx, Wh, stateful=False):\n",
        "        self.Wx, self.Wh = Wx, Wh\n",
        "        selfdWx, self.dWh = None, None\n",
        "        self.layers = None\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T, D = xs.shape\n",
        "        H, H3 = self.Wh.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = GRU(self.Wx, self.Wh)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        N, T, H = dhs.shape\n",
        "        D = self.Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        self.dWx, self.dWh = 0, 0\n",
        "\n",
        "        dh = 0\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "\n",
        "            dxs[:, t, :] = dx\n",
        "            self.dWx += layer.dWx\n",
        "            self.dWh += layer.dWh\n",
        "\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "\n",
        "\n",
        "class Simple_TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "        layers = []\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = SoftmaxWithLoss()\n",
        "            loss += layer.forward(xs[:, t, :], ts[:, t])\n",
        "            layers.append(layer)\n",
        "        loss /= T\n",
        "\n",
        "        self.cache = (layers, xs)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        layers, xs = self.cache\n",
        "        N, T, V = xs.shape\n",
        "        dxs = np.empty(xs.shape, dtype='f')\n",
        "\n",
        "        dout *= 1/T\n",
        "        for t in range(T):\n",
        "            layer = layers[t]\n",
        "            dxs[:, t, :] = layer.backward(dout)\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "class Simple_TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W, self.b = W, b\n",
        "        self.dW, self.db = None, None\n",
        "        self.layers = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T, D = xs.shape\n",
        "        D, M = self.W.shape\n",
        "\n",
        "        self.layers = []\n",
        "        out = np.empty((N, T, M), dtype='f')\n",
        "        for t in range(T):\n",
        "            layer = Affine(self.W, self.b)\n",
        "            out[:, t, :] = layer.forward(xs[:, t, :])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, M = dout.shape\n",
        "        D, M = self.W.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        self.dW, self.db = 0, 0\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dxs[:, t, :] = layer.backward(dout[:, t, :])\n",
        "\n",
        "            self.dW += layer.dW\n",
        "            self.db += layer.db\n",
        "\n",
        "        return dxs\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSzVu5dkcMLt",
        "colab_type": "text"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYFuzfA_cQos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from np import *\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        x = np.exp(x)\n",
        "        x /= x.sum(axis=1, keepdims=True)\n",
        "    elif x.ndim == 1:\n",
        "        x = x - np.max(x)\n",
        "        x = np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRp1u1IZcTuW",
        "colab_type": "text"
      },
      "source": [
        "# layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlnUsupIcVeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#from np import *  # import numpy as np\n",
        "#from config import GPU\n",
        "#from functions import softmax, cross_entropy_error\n",
        "\n",
        "\n",
        "class MatMul:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out = np.dot(x, W) + b\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, b = self.params\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = softmax(x)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = self.out * dout\n",
        "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "        dx -= self.out * sumdx\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.y = None  # softmaxの出力\n",
        "        self.t = None  # 教師ラベル\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "\n",
        "        # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
        "        if self.t.size == self.y.size:\n",
        "            self.t = self.t.argmax(axis=1)\n",
        "\n",
        "        loss = cross_entropy_error(self.y, self.t)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = self.y.copy()\n",
        "        dx[np.arange(batch_size), self.t] -= 1\n",
        "        dx *= dout\n",
        "        dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SigmoidWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.loss = None\n",
        "        self.y = None  # sigmoidの出力\n",
        "        self.t = None  # 教師データ\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = 1 / (1 + np.exp(-x))\n",
        "\n",
        "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        dx = (self.y - self.t) * dout / batch_size\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    '''\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    '''\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.params, self.grads = [], []\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "\n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "        np.add.at(dW, self.idx, dout)\n",
        "        return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD0tflqRdIYs",
        "colab_type": "text"
      },
      "source": [
        "# rnnlm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lalmP3BEdL-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#import sys\n",
        "#sys.path.append('..')\n",
        "#from common.time_layers import *\n",
        "#from common.base_model import BaseModel\n",
        "\n",
        "\n",
        "class Rnnlm(BaseModel):\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 重みの初期化\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layer = self.layers[1]\n",
        "\n",
        "        # すべての重みと勾配をリストにまとめる\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.lstm_layer.reset_state()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VV4JR4LdU8s",
        "colab_type": "text"
      },
      "source": [
        "# train rnnlm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XYqofYhdXrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "236acea0-c48a-417e-8d07-4f0675ef6eb7"
      },
      "source": [
        "# coding: utf-8\n",
        "#import sys\n",
        "#sys.path.append('..')\n",
        "#from common.optimizer import SGD\n",
        "#from common.trainer import RnnlmTrainer\n",
        "#from common.util import eval_perplexity\n",
        "#from dataset import ptb\n",
        "#from rnnlm import Rnnlm\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "batch_size = 20\n",
        "wordvec_size = 100\n",
        "hidden_size = 100  # RNNの隠れ状態ベクトルの要素数\n",
        "time_size = 35  # RNNを展開するサイズ\n",
        "lr = 20.0\n",
        "max_epoch = 4\n",
        "max_grad = 0.25\n",
        "\n",
        "# 学習データの読み込み\n",
        "corpus, word_to_id, id_to_word = load_data('train')\n",
        "corpus_test, _, _ = load_data('test')\n",
        "vocab_size = len(word_to_id)\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1:]\n",
        "\n",
        "# モデルの生成\n",
        "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "# 勾配クリッピングを適用して学習\n",
        "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
        "            eval_interval=20)\n",
        "trainer.plot(ylim=(0, 500))\n",
        "\n",
        "# テストデータで評価\n",
        "model.reset_state()\n",
        "ppl_test = eval_perplexity(model, corpus_test)\n",
        "print('test perplexity: ', ppl_test)\n",
        "\n",
        "# パラメータの保存\n",
        "model.save_params()\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch 1 |  iter 1 / 1327 | time 0[s] | perplexity 10000.59\n",
            "| epoch 1 |  iter 21 / 1327 | time 7[s] | perplexity 2936.76\n",
            "| epoch 1 |  iter 41 / 1327 | time 14[s] | perplexity 1232.32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6bbd0d424df3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 勾配クリッピングを適用して学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n\u001b[0;32m---> 23\u001b[0;31m             eval_interval=20)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-b7fe1accf919>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# 勾配を求め、パラメータを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-b7fe1accf919>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, x, t, batch_size, time_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0mbatch_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mWmZsh1d1TD",
        "colab_type": "text"
      },
      "source": [
        "# trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lokgl1Uzd6Mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "#import sys\n",
        "#sys.path.append('..')\n",
        "#import numpy\n",
        "#import time\n",
        "#import matplotlib.pyplot as plt\n",
        "#from np import *  # import numpy as np\n",
        "#from util import clip_grads\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_list = []\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "        data_size = len(x)\n",
        "        max_iters = data_size // batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            # シャッフル\n",
        "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
        "            x = x[idx]\n",
        "            t = t[idx]\n",
        "\n",
        "            for iters in range(max_iters):\n",
        "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "                # 勾配を求め、パラメータを更新\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # 評価\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    avg_loss = total_loss / loss_count\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
        "                    self.loss_list.append(float(avg_loss))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = numpy.arange(len(self.loss_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.loss_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('loss')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class RnnlmTrainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.time_idx = None\n",
        "        self.ppl_list = None\n",
        "        self.eval_interval = None\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def get_batch(self, x, t, batch_size, time_size):\n",
        "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
        "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "\n",
        "        data_size = len(x)\n",
        "        jump = data_size // batch_size\n",
        "        offsets = [i * jump for i in range(batch_size)]  # バッチの各サンプルの読み込み開始位置\n",
        "\n",
        "        for time in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
        "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
        "            self.time_idx += 1\n",
        "        return batch_x, batch_t\n",
        "\n",
        "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
        "            max_grad=None, eval_interval=20):\n",
        "        data_size = len(xs)\n",
        "        max_iters = data_size // (batch_size * time_size)\n",
        "        self.time_idx = 0\n",
        "        self.ppl_list = []\n",
        "        self.eval_interval = eval_interval\n",
        "        model, optimizer = self.model, self.optimizer\n",
        "        total_loss = 0\n",
        "        loss_count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "        for epoch in range(max_epoch):\n",
        "            for iters in range(max_iters):\n",
        "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
        "\n",
        "                # 勾配を求め、パラメータを更新\n",
        "                loss = model.forward(batch_x, batch_t)\n",
        "                model.backward()\n",
        "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
        "                if max_grad is not None:\n",
        "                    clip_grads(grads, max_grad)\n",
        "                optimizer.update(params, grads)\n",
        "                total_loss += loss\n",
        "                loss_count += 1\n",
        "\n",
        "                # パープレキシティの評価\n",
        "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "                    ppl = np.exp(total_loss / loss_count)\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print('| epoch %d |  iter %d / %d | time %d[s] | perplexity %.2f'\n",
        "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
        "                    self.ppl_list.append(float(ppl))\n",
        "                    total_loss, loss_count = 0, 0\n",
        "\n",
        "            self.current_epoch += 1\n",
        "\n",
        "    def plot(self, ylim=None):\n",
        "        x = numpy.arange(len(self.ppl_list))\n",
        "        if ylim is not None:\n",
        "            plt.ylim(*ylim)\n",
        "        plt.plot(x, self.ppl_list, label='train')\n",
        "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "        plt.ylabel('perplexity')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def remove_duplicate(params, grads):\n",
        "    '''\n",
        "    パラメータ配列中の重複する重みをひとつに集約し、\n",
        "    その重みに対応する勾配を加算する\n",
        "    '''\n",
        "    params, grads = params[:], grads[:]  # copy list\n",
        "\n",
        "    while True:\n",
        "        find_flg = False\n",
        "        L = len(params)\n",
        "\n",
        "        for i in range(0, L - 1):\n",
        "            for j in range(i + 1, L):\n",
        "                # 重みを共有する場合\n",
        "                if params[i] is params[j]:\n",
        "                    grads[i] += grads[j]  # 勾配の加算\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "                # 転置行列として重みを共有する場合（weight tying）\n",
        "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
        "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
        "                    grads[i] += grads[j].T\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "\n",
        "                if find_flg: break\n",
        "            if find_flg: break\n",
        "\n",
        "        if not find_flg: break\n",
        "\n",
        "    return params, grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}